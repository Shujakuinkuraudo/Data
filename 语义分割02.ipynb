{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\ndef rle_encode(im):\n    pixels = im.flatten(order = 'F')\n    '''\n    a =  np.array([[1,1,1,0],      \n                   [0,1,1,1], \n                   [1,1,0,1]])\n    a = a.flatten(order = 'F')\n    #[1 0 1 1 1 1 1 1 0 0 1 1]\n\n    '''\n    pixels = np.concatenate([[0],pixels,[0]])\n    '''\n    左右各扩增一位0\n    a_1 = np.concatenate([[0],a,[0]])\n    #[0 1 0 1 1 1 1 1 1 0 0 1 1 0]\n    '''\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    '''\n    在第几位0和1发生改变\n    runs = np.where(a_1[1:] != a_1[:-1])[0]+1\n    #[ 1  2  3  9 11 13]\n    '''\n    runs[1::2] -= runs[::2]\n    '''\n    奇数位表示第几位为1偶数位表示持续多少个\n    runs[1::2] -= runs[::2]\n    #[ 1  1  3  6 11  2]\n    '''\n    return ' '.join(str(x) for x in runs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_decode(mask_rle,shape=(512,512)):\n    s = mask_rle.split()\n    starts,lengths = [np.asarray(x,dtype=int) for x in(s[0:][::2],s[1:][::2])]\n    starts -=1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1],dtype=np.uint8)\n    for lo,hi in zip(starts,ends):\n        img[lo:hi] = 1\n    return img.reshape(shape,order='F')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport cv2\ntrain_mask = pd.read_csv('../input/train-mask/train_mask.csv',sep='\\t',names=['name','mask'])\n# 读取第一张图，并将对于的rle解码为mask矩阵\ncount = 0\npercent = [0]*len(train_mask)\nfor c in range(0,len(train_mask)):\n    if type(train_mask['mask'].iloc[c]) == float:\n        count += 1 \n    else:\n        mask1 = rle_decode(train_mask['mask'].iloc[c])\n        count_1 = np.sum(mask1 == 1)\n        count_0 = np.sum(mask1 == 0)\n        percent[c] = count_1/(count_0+count_1)\n\nprint('无障碍图片所占比例%f%%'%(count/len(train_mask)*100))\nprint(percent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import Image as IMG\nfrom matplotlib import pyplot as plt\nmask_0 = rle_decode(train_mask['mask'].iloc[0])\nnew_im = Image.fromarray(mask_0)\nplt.imshow(new_im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for value in percent:\n    value_sum = 0\n    value_sum += value*100 \nvalue_average = value_sum/len(train_mask)\nprint('建筑物所占区平均百分比为%f%%'%(value_average*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimg = cv2.imread(\"../input/traindata/train/\"+train_mask['name'].iloc[0])\nmask =rle_decode(train_mask['mask'].iloc[0])\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#垂直翻转\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplt.imshow(cv2.flip(img,0))\n\nplt.subplot(1,2,2)\nplt.imshow(cv2.flip(mask,0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#水平翻转\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplt.imshow(cv2.flip(img,1))\n\nplt.subplot(1,2,2)\nplt.imshow(cv2.flip(mask,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#随机裁剪\nx,y = np.random.randint(0,256), np.random.randint(0,256)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplt.imshow(img[x:x+256,y:y+256])\n\nplt.subplot(1,2,2)\nplt.imshow(mask[x:x+256,y:y+256])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import albumentations as A\n\n#水平翻转\naugments = A.HorizontalFlip(p=1)(image=img,mask=mask)\nimg_aug,mask_aug = augments['image'], augments['mask']\n\n#随机剪裁\naugments = A.RandomCrop(p=1,height=256,width=256)(image=img,mask=mask)\n#旋转\naugments = A.shiftScaleRotate(p=1)(image=img,mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trfm= A.Compose([\n    #https://blog.csdn.net/qq_33499229/article/details/108316386\n    A.Resize(256,256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90()\n    A.Rotate(limit=89,p=0.5)\n])\n\naugments = trfm(image=img,mask=mask)\nimg_aug,mask_aug = augments['image'], augments['mask']\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplt.imshow(img_aug)\nplt.subplot(1,2,2)\nplt.imshow(mask_aug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data as D\ntrain_mask = pd.read_csv('../input/train-mask/train_mask.csv',sep='\\t',names = ['name',\"mask\"])\nclass TianchiDataset(D.Dataset):\n    def __init__(self,path,rles,transform):\n        self.path = path\n        self.rles = rles\n        self.transform = transform\n        self.len = len(path)\n    def __getitem__(self,index):\n        img = cv2.imread(self.path[index])\n        mask = rle_decode(self.rls[index])\n        augments = self.transform(image = img,mask=mask)\n        return self.as_tensor(augments['image']), augments['mask'][None]\n    def __len__(self):\n        return self.len\n    \ndataset = TianchiDataset(\n    \"../input/traindata/train/\"+train_mask['name'].values,\n    train_mask['mask'].fillna('').values,\n    trfm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loader = D.DataLoader(dataset,batch_size=10,shuffle=True,num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pathlib, sys, os, random, time\nimport numba, cv2, gc\nfrom tqdm import tqdm_notebook\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.notebook import tqdm\n\nimport albumentations as A\n\nimport rasterio\nfrom rasterio.windows import Window\n\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(512, 512)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape, order='F')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as D\n\nimport torchvision\nfrom torchvision import transforms as T\n\nEPOCHES = 20\nBATCH_SIZE = 48\nIMAGE_SIZE = 256\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu' \n\ntrfm = A.Compose([\n    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(),\n])\n\nclass TianChiDataset(D.Dataset):\n    def __init__(self, paths, rles, transform, test_mode=False):\n        self.paths = paths\n        self.rles = rles\n        self.transform = transform\n        self.test_mode = test_mode\n        \n        self.len = len(paths)\n        self.as_tensor = T.Compose([\n            T.ToPILImage(),\n            T.Resize(IMAGE_SIZE),\n            T.ToTensor(),\n            T.Normalize([0.625, 0.448, 0.688],\n                        [0.131, 0.177, 0.101]),\n        ])\n        \n    # get data operation\n    def __getitem__(self, index):\n        img = cv2.imread(self.paths[index])\n        if not self.test_mode:\n            mask = rle_decode(self.rles[index])\n            augments = self.transform(image=img, mask=mask)\n            return self.as_tensor(augments['image']), augments['mask'][None]\n        else:\n            return self.as_tensor(img), ''        \n    \n    def __len__(self):\n        \"\"\"\n        Total number of samples in the dataset\n        \"\"\"\n        return self.len\ntrain_mask = pd.read_csv('../input/train-mask/train_mask.csv', sep='\\t', names=['name', 'mask'])\ntrain_mask['name'] = train_mask['name'].apply(lambda x: '../input/traindata/train/' + x)\n\nimg = cv2.imread(train_mask['name'].iloc[0])\nmask = rle_decode(train_mask['mask'].iloc[0])\n\nprint(rle_encode(mask) == train_mask['mask'].iloc[0])\ndataset = TianChiDataset(\n    train_mask['name'].values,\n    train_mask['mask'].fillna('').values,\n    trfm, False\n)\nimage, mask = dataset[0]\nplt.figure(figsize=(16,8))\nplt.subplot(121)\nplt.imshow(mask[0], cmap='gray')\nplt.subplot(122)\nplt.imshow(image[0])\n\nvalid_idx, train_idx = [], []\nfor i in range(len(dataset)):\n    if i % 7 == 0:\n        valid_idx.append(i)\n#     else:\n    elif i % 7 == 1:\n        train_idx.append(i)\n        \ntrain_ds = D.Subset(dataset, train_idx)\nvalid_ds = D.Subset(dataset, valid_idx)\n\n# define training and validation data loaders\nloader = D.DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\nvloader = D.DataLoader(\n    valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ndef get_model():\n    model = torchvision.models.segmentation.fcn_resnet50(True)\n    \n#     pth = torch.load(\"../input/pretrain-coco-weights-pytorch/fcn_resnet50_coco-1167a1af.pth\")\n#     for key in [\"aux_classifier.0.weight\", \"aux_classifier.1.weight\", \"aux_classifier.1.bias\", \"aux_classifier.1.running_mean\", \"aux_classifier.1.running_var\", \"aux_classifier.1.num_batches_tracked\", \"aux_classifier.4.weight\", \"aux_classifier.4.bias\"]:\n#         del pth[key]\n    \n    model.classifier[4] = nn.Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n    return model\n\n@torch.no_grad()\ndef validation(model, loader, loss_fn):\n    losses = []\n    model.eval()\n    for image, target in loader:\n        image, target = image.to(DEVICE), target.float().to(DEVICE)\n        output = model(image)['out']\n        loss = loss_fn(output, target)\n        losses.append(loss.item())\n        \n    return np.array(losses).mean()\nmodel = get_model()\nmodel.to(DEVICE);\n\noptimizer = torch.optim.AdamW(model.parameters(),\n                  lr=1e-4, weight_decay=1e-3)\n\nclass SoftDiceLoss(nn.Module):\n    def __init__(self, smooth=1., dims=(-2,-1)):\n\n        super(SoftDiceLoss, self).__init__()\n        self.smooth = smooth\n        self.dims = dims\n    \n    def forward(self, x, y):\n        tp = (x * y).sum(self.dims)\n        fp = (x * (1 - y)).sum(self.dims)\n        fn = ((1 - x) * y).sum(self.dims)\n        \n        dc = (2 * tp + self.smooth) / (2 * tp + fp + fn + self.smooth)\n        dc = dc.mean()\n        return 1 - dc\n    \nbce_fn = nn.BCEWithLogitsLoss()\ndice_fn = SoftDiceLoss()\n\ndef loss_fn(y_pred, y_true):\n    bce = bce_fn(y_pred, y_true)\n    dice = dice_fn(y_pred.sigmoid(), y_true)\n    return 0.8*bce+ 0.2*dice\nheader = r'''\n        Train | Valid\nEpoch |  Loss |  Loss | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*2 + '\\u2502{:6.2f}'\nprint(header)\n\nEPOCHES = 10\nbest_loss = 10\nfor epoch in range(1, EPOCHES+1):\n    losses = []\n    start_time = time.time()\n    model.train()\n    for image, target in tqdm_notebook(loader):\n        \n        image, target = image.to(DEVICE), target.float().to(DEVICE)\n        optimizer.zero_grad()\n        output = model(image)['out']\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        # print(loss.item())\n        \n    vloss = validation(model, vloader, loss_fn)\n    print(raw_line.format(epoch, np.array(losses).mean(), vloss,\n                              (time.time()-start_time)/60**1))\n    losses = []\n    \n    if vloss < best_loss:\n        best_loss = vloss\n        torch.save(model.state_dict(), 'model_best.pth')\ntrfm = T.Compose([\n    T.ToPILImage(),\n    T.Resize(IMAGE_SIZE),\n    T.ToTensor(),\n    T.Normalize([0.625, 0.448, 0.688],\n                [0.131, 0.177, 0.101]),\n])\n\nsubm = []\n\nmodel.load_state_dict(torch.load(\"./model_best.pth\"))\nmodel.eval()\ntest_mask = pd.read_csv('../input/submission/test_a_samplesubmit.csv', sep='\\t', names=['name', 'mask'])\ntest_mask['name'] = test_mask['name'].apply(lambda x: '../input/testdata/test_a/' + x)\n\nfor idx, name in enumerate(tqdm_notebook(test_mask['name'].iloc[:])):\n    image = cv2.imread(name)\n    image = trfm(image)\n    with torch.no_grad():\n        image = image.to(DEVICE)[None]\n        score = model(image)['out'][0][0]\n        score_sigmoid = score.sigmoid().cpu().numpy()\n        score_sigmoid = (score_sigmoid > 0.5).astype(np.uint8)\n        score_sigmoid = cv2.resize(score_sigmoid, (512, 512))\n\n        \n        # break\n    subm.append([name.split('/')[-1], rle_encode(score_sigmoid)])\nsubm = pd.DataFrame(subm)\nsubm.to_csv('./tmp.csv', index=None, header=None, sep='\\t')\nplt.figure(figsize=(16,8))\nplt.subplot(121)\nplt.imshow(rle_decode(subm[1].fillna('').iloc[0]), cmap='gray')\nplt.subplot(122)\nplt.imshow(cv2.imread('../input/testdata/test_a/' + subm[0].iloc[0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trfm = T.Compose([\n    T.ToPILImage(),\n    T.Resize(IMAGE_SIZE),\n    T.ToTensor(),\n    T.Normalize([0.625, 0.448, 0.688],\n                [0.131, 0.177, 0.101]),\n])\n\nsubm = []\n\nmodel.load_state_dict(torch.load(\"./model_best.pth\"))\nmodel.eval()\ntest_mask = pd.read_csv('../input/submission/test_a_samplesubmit.csv', sep='\\t', names=['name', 'mask'])\ntest_mask['name'] = test_mask['name'].apply(lambda x: '../input/testdata/test_a/' + x)\n\nfor idx, name in enumerate(tqdm_notebook(test_mask['name'].iloc[:])):\n    image = cv2.imread(name)\n    image = trfm(image)\n    with torch.no_grad():\n        image = image.to(DEVICE)[None]\n        score = model(image)['out'][0][0]\n        score_sigmoid = score.sigmoid().cpu().numpy()\n        score_sigmoid = (score_sigmoid > 0.5).astype(np.uint8)\n        score_sigmoid = cv2.resize(score_sigmoid, (512, 512))\n\n        \n        # break\n    subm.append([name.split('/')[-1], rle_encode(score_sigmoid)])\nsubm = pd.DataFrame(subm)\nsubm.to_csv('./tmp.csv', index=None, header=None, sep='\\t')\nplt.figure(figsize=(16,8))\nplt.subplot(121)\nplt.imshow(rle_decode(subm[1].fillna('').iloc[0]), cmap='gray')\nplt.subplot(122)\nplt.imshow(cv2.imread('../input/testdata/test_a/' + subm[0].iloc[0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}